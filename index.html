<!doctype html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
    <title>Standards for Media and Real Time Communications on the Web</title>
    <link rel="stylesheet" href="http://www.w3.org/2007/08/video/style.css" type="text/css"/>
    <link rel="stylesheet" href="http://www.w3.org/2007/08/video/print.css" type="text/css" media="print"/>
    <link rel="stylesheet" href="style.css" type="text/css"/>
    <meta name="author" content="Dominique Hazael-Massieux"/>
    <meta name="twitter:card" content="summary"/>
    <meta name="twitter:site" content="@w3c"/>
    <meta name="twitter:creator" content="@dontcallmedom"/>
    <style>table { width:100%;}
      td { text-align: left;}
    </style>
  </head>
  <body>
    <div id="page">
      <h1>Standards for Media and Real Time Communications on the Web</h1>
      <h2 id="month"></h2>
    </div>
    <div id="main">
      <p class="logo">
        <a href="/"><img alt="W3C" src="//www.w3.org/Icons/w3c_home"/></a>
      </p>
<!--      <dl class="versions">
        <dt>Latest version</dt>
        <dd>
        </dd>
        <dt>This version</dt>
        <dd id="this"></dd>
        <dt>Previous version</dt>
        <dd id="prev"></dd>
      </dl>-->
      <section id="intro">
        <h2>Introduction</h2>

        <p>This document provides an overview of Web technologies as they can be used to build media and real-time communication services, and highlights some of the known gaps to enable additional use cases.</p>
        <p>It organizes these technologies based on the features they enabled, these features themselves being grouped into 6 main categories:</p>
        <dl>
          <dt><a href="#rendering">media rendering</a></dt>
          <dd>features needed to render media content on one or more devices;</dd>
          <dt><a href="#processing">media processing</a></dt>
          <dd>features needed to analyze or modify media content;</dd>
          <dt><a href="#sync">synchronized content</a></dt>
          <dd>features needed to synchronize multiple media or non-media content together, on one or several devices;</dd>
          <dt><a href="#control">media control</a></dt>
          <dd>features needed to let the user interact with the playback of media content, both via local and remote interactions;</dd>
          <dt><a href="#transmission">media transmission</a></dt>
          <dd>features that help with transmitting media content from its source to its rendering target;</dd>
          <dt><a href="#capture">media capture</a></dt>
          <dd>features needed to capture media content from a variety of available sources.</dd>
        </dl>
        <p>In each of these categories, we describe the specifications that enable these features under 4 main maturity levels:</p>
        <dl>
          <dt>Well-deployed technologies</dt>
          <dd>Technologies that have ready or mostly ready specifications, and that are relatively well deployed in modern browsers</dd>
          <dt>Specifications in progress</dt>
          <dd>Technologies for which a draft is available and there are existing implementations or fairly clear intents to implement</dd>
          <dt>Exploratory work</dt>
          <dd>Early specifications that define a possible approach to solve a problem and where intents to implement are either unclear or inexistant yet</dd>
          <dt>Featured not covered by ongoing work</dt>
          <dd>Gaps in the features that are currently addressed by known specifications</dd>
        </dl>
        <p>A critical goal of this document is seek further input on what additional gaps exist in the Web platform in the space of media and real-time communications.</p>
        <p>Readers are strongly encouraged to <strong>submit their use cases</strong> that cannot be achieved today with Web technologies, either by starting a new topic in the <a href="http://discourse.specifiction.org/c/mediartc">Media and Real-Time Communications category of W3Câ€™s discourse forum</a> or by raising an <a href="https://github.com/dontcallmedom/mediartc-roadmap/issues/new">issue on the Github repository</a> where this document is maintained.</p>
      </section>
      <script type="text/template" id="template-deployed">
  <table>
    <thead>
      <tr>
        <th>Feature</th>
        <th>Specification</th>
        <th>Working Group</th>
        <th>Maturity</th>
        <th>Stability</th>
        <th>Current implementations</th>
      </tr>
    </thead>
    <tbody>
    </tbody>
  </table>

      </script>
      <script type="text/template" id="template-explore">
  <table>
    <thead>
      <tr>
        <th>Feature</th>
        <th>Specification</th>
        <th>Group</th>
        <th>Implementation intents</th>
      </tr>
    </thead>
    <tbody>
    </tbody>
  </table>

      </script>
      <section>
        <h2>Media Rendering</h2>
        <section class="featureset deployed">
          <h3>Well-deployed technologies</h3>
          <p>Few media-based services can usefully function without rendering audio or video content; the HTML5 specification provides widely deployed support for this essential feature:</p>
          <ul>
            <li data-feature="Audio rendering">audio content can be rendered in any Web page via the <a data-featureid="audio"><code>&lt;audio&gt;</code> element</a></li>
            <li data-feature="Video rendering">likewise, video content can be rendered with <a data-featureid="video"><code>&lt;video&gt;</code> element</a></li>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Audio rendering">Beyond the declarative approach enabled by the <code>&lt;audio&gt;</code> element, the <a data-featureid="webaudio">Web Audio API</a> provides a full-featured audio processing API, which includes support for low-latency plack of audio content.</p>
          <div data-feature="Distributed rendering">
            <p>As users increasingly own more and more connected devices, the need to get these devices to work together increases as well.</p>
            <p>The <a data-featureid="secondscreen">Second Screen Presentation API</a> offers the possibility for a Web page to open and control a page located on another screen, opening the road for multi-screen Web applications.</p>
            <p>The <a data-featureid="audio-output">Audio Output Devices API</a> offer a similar functionality for audio streams, enabling a Web application to pick on which audio output devices a given sound should be played.</p>
          </div>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <div data-feature="Distributed rendering">
            <p>The <a data-featureid="discovery">Network Service Discovery API</a> offers a lower-level approach to the establishement of multi-device operations, by providing integration with local network-based media renderers, such as those enabled by DLNA, UPnP, etc. At this time, the future of this specification is uncertain, given the difficulty of finding the right balance between the requirements of the Web security model and the desire to be compatible with existing devices.</p>
            <p>The Multi-Device Timing Community Group is exploring another aspect of multi-device media rendering: its <a data-featureid="timing">Timing Object</a> specification enables to keep video, audio and other data streams in close synchrony, across devices and independently of the network topology.</p>
            </div>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>Color Management</dt>
            <dd>To ensure the proper rendering of videos with high-dynamic range and wide-gamut colors, content providers would need to determine whether the underlying device and browser have proper support for this.</dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>Media Processing</h2>
        <section class="featureset deployed">
          <h3>Well-deployed technologies</h3>
          <p data-feature="Image and Video Processing"><a data-featureid="canvas">Canvas API</a></p>
          <p data-feature="Video inserts"><a data-featureid="mse">Media Stream Extensions</a></p>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Audio Processing"><a data-featureid="webaudio">Web Audio API</a></p>

          <p data-feature="Protected Media"><a data-featureid="eme">Encrypted Media Extensions</a></p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="Video processing"><a data-featureid="videoworker">Worker for harwarde-accelerated video processing</a>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>JavaScript-based Codecs</dt>
            <dd></dd>
            <dt>Content Decryption Module API</dt>
            <dd></dd>
            <dt>Conditional Access System</dt>
            <dd></dd>
            <dt>Hardware-accelerated video analysis</dt>
            <dd>Face and object recognition [WebCL]</dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>Synchronized content</h2>
        <section class="featureset deployed">
          <h3>Well-deployed technologies</h3>
          <p data-feature="Timeline management"><a data-featureid="timeupdate"><code>timeupdate</code> event</a></p>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Close Captioning"><a data-featureid="webvtt">Web VTT</a> <a data-featureid="ttml">TTML 2</a></p>
          <p data-feature="Timeline management"><a data-featureid="webanimations">Web Animations</a></p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="Close Captioning"><a data-featureid="inband">In-band track sourcing</a> </p>
          <p data-feature="Transcripts"><a data-featureid="transcript">Transcript extension</a></p>
          <p data-feature="Multi-device synchronization"><a data-featureid="timing">Timing Object</a></p>
        </section>
      </section>
      <section>
        <h2>Media Control</h2>
        <section class="featureset deployed">
        <h3>Well-deployed technologies</h3>
        <p data-feature="Local media control"><a data-featureid="htmlmediaelement"><code>HTMLMediaElement</code> interface</a></p>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Key-based control"><a data-featureid="mediakeys">Media Keys in DOM Level 3 events</a></p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="OS-wide media control"><a data-featureid="mediasession">Media session</a></p>
          <p data-feature="Tuner control"><a data-featureid="tvtuner">Tuner interface in TV Control API</a></p>
          <p data-feature="Virtual reality control"><a data-featureid="webvr">WebVR</a></p>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>Timeshifted broadcast</dt>
            <dd></dd>
            <dt>Radio Tuner Control</dt>
            <dd></dd>
        </section>
      </section>
      <section>
        <h2>Media Transmission</h2>
        <section class="featureset deployed">
        <h3>Well-deployed technologies</h3>
        <p data-feature="Adaptive Video Streaming"><a data-featureid="mse">Media Stream Extensions</a></p>
        </section>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="P2P transmission"><a data-featureid="p2p">peer-to-peer transmission via the WebRTC API</a></p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="Broadcast"><a data-featureid="tvtuner">Tuner API in TV Control specification</a></p>
        </section>
      </section>
      <section>
        <h2>Media Capture</h2>
        <section class="featureset deployed">
          <h3>Specifications in progress</h3>
          <p data-feature="Camera and mike capture"><a data-featureid="getusermedia">Media Capture and Streams API</a></p>
          <p data-feature="Screen capture"><a data-featureid="domcapture">Media Capture from DOM elements</a></p>
          <p data-feature="Recording"><a data-featureid="recording">Media Recorder API</a></p>
        </section>
        <section class="featureset explore">
          <h3>Exploratory work</h3>
          <p data-feature="Photography API"><a data-featureid="imagecapture">Mediastream Image Capture</a></p>
          <p data-feature="3D Camera capture"><a data-featureid="3dcamera">Media Capture Depth Stream Extensions</a></p>
        </section>
        <section>
          <h3>Featured not covered by ongoing work</h3>
          <dl>
            <dt>Audio output capture</dt>
            <dd></dd>
            <dt>Timed textual alternative</dt>
            <dd></dd>
          </dl>
        </section>
      </section>
      <section>
        <h2>Acknowledgments</h2>
        <p>This document is produced through the <a href="http://mediascapeproject.eu/">MediaScape project</a>, funded by the European Union through the Seventh Framework Programme (FP7/2013-2016) under grant agreement nÂ°610404.</p>
      </section>
      <div id="footer">
        <address><a href="http://www.w3.org/People/Dom/">Dominique
HazaÃ«l-Massieux</a> &lt;<a href="mailto:dom@w3.org">dom@w3.org</a>&gt; / <a href="https://twitter.com/dontcallmedom">@dontcallmedom</a></address>
      </div>
    </div>
    <script src="js/generate.js"></script>
  </body>
</html>
